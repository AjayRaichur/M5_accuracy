{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from downcast import reduce\n",
    "import pickle\n",
    "import warnings\n",
    "from catboost import CatBoostRegressor\n",
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_pickle(\"sales_ad.pkl\")\n",
    "cal = pd.read_pickle(\"cal_ad.pkl\")\n",
    "price = pd.read_pickle(\"prices_ad.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance metric \n",
    "\n",
    "class WRMSSEEvaluator(object):\n",
    "\n",
    "    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n",
    "        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n",
    "        train_target_columns = train_y.columns.tolist()\n",
    "        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n",
    "\n",
    "        train_df['all_id'] = 0  # for lv1 aggregation\n",
    "\n",
    "        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n",
    "        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n",
    "\n",
    "        if not all([c in valid_df.columns for c in id_columns]):\n",
    "            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n",
    "\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.calendar = calendar\n",
    "        self.prices = prices\n",
    "\n",
    "        self.weight_columns = weight_columns\n",
    "        self.id_columns = id_columns\n",
    "        self.valid_target_columns = valid_target_columns\n",
    "\n",
    "        weight_df = self.get_weight_df()\n",
    "\n",
    "        self.group_ids = (\n",
    "            'all_id',\n",
    "            'state_id',\n",
    "            'store_id',\n",
    "            'cat_id',\n",
    "            'dept_id',\n",
    "            ['state_id', 'cat_id'],\n",
    "            ['state_id', 'dept_id'],\n",
    "            ['store_id', 'cat_id'],\n",
    "            ['store_id', 'dept_id'],\n",
    "            'item_id',\n",
    "            ['item_id', 'state_id'],\n",
    "            ['item_id', 'store_id']\n",
    "        )\n",
    "\n",
    "        for i, group_id in enumerate(tqdm(self.group_ids)):\n",
    "            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n",
    "            scale = []\n",
    "            for _, row in train_y.iterrows():\n",
    "                series = row.values[np.argmax(row.values != 0):]\n",
    "                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n",
    "            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n",
    "            setattr(self, f'lv{i + 1}_train_df', train_y)\n",
    "            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n",
    "\n",
    "            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n",
    "            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n",
    "\n",
    "    def get_weight_df(self) -> pd.DataFrame:\n",
    "        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n",
    "        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n",
    "        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n",
    "        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n",
    "\n",
    "        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n",
    "        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n",
    "        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n",
    "        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n",
    "        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n",
    "        return weight_df\n",
    "\n",
    "    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n",
    "        valid_y = getattr(self, f'lv{lv}_valid_df')\n",
    "        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n",
    "        scale = getattr(self, f'lv{lv}_scale')\n",
    "        return (score / scale).map(np.sqrt)\n",
    "\n",
    "    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n",
    "        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n",
    "\n",
    "        if isinstance(valid_preds, np.ndarray):\n",
    "            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n",
    "\n",
    "        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n",
    "\n",
    "        all_scores = []\n",
    "        for i, group_id in enumerate(self.group_ids):\n",
    "            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n",
    "            weight = getattr(self, f'lv{i + 1}_weight')\n",
    "            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n",
    "            all_scores.append(lv_scores.sum())\n",
    "\n",
    "        return np.mean(all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_1(data_point):\n",
    "    if type(data_point) != pd.core.frame.DataFrame:\n",
    "        data_point = pd.DataFrame(data_point,columns = sales.columns)\n",
    "    \n",
    "    d_cols = [d for d in sales.columns if 'd_' in d]\n",
    "    data = data_point.drop(d_cols[:1883],axis = 1)\n",
    "    \n",
    "    #making cols for days 1942-69 amd filling it with zero\n",
    "    for day in range(1942,1970):\n",
    "        data['d_' + str(day)] = 0\n",
    "        data['d_' + str(day)] = data['d_' + str(day)].astype(np.int16)\n",
    "    \n",
    "    pre_data = pd.melt(data, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
    "                      var_name='d', value_name='sales')\n",
    "    \n",
    "    #combining the dataset \n",
    "    pre_data = pd.merge(pre_data, cal, on='d', how='left')\n",
    "    pre_data = pd.merge(pre_data, price, on=['store_id','item_id','wm_yr_wk'], how='left')\n",
    "    \n",
    "    #fil the missing sell price values by mean imputaion\n",
    "    pre_data[\"sell_price\"].fillna(pre_data.groupby(\"id\")[\"sell_price\"].transform(\"mean\"), inplace=True)\n",
    "    \n",
    "    pre_data.drop(columns=[\"date\",\"weekday\"], inplace=True)\n",
    "    pre_data['d'] = pre_data['d'].apply(lambda a: a.split('_')[1]).astype(np.int16)\n",
    "\n",
    "    #calculating lags feature\n",
    "    lags = [1,2,3,5,7,14,21,28]\n",
    "    for lag in lags:\n",
    "        pre_data[\"lag_\" + str(lag)] = pre_data.groupby(\"id\")[\"sales\"].shift(lag).astype(np.float16)\n",
    "\n",
    "    #calculating rolling features\n",
    "    pre_data['rolling_mean_10'] = pre_data.groupby(\"id\")['sales'].transform(lambda x: x.rolling(10).mean())\n",
    "    pre_data['rolling_mean_20'] = pre_data.groupby(\"id\")['sales'].transform(lambda x: x.rolling(20).mean())\n",
    "    pre_data['rolling_mean_30'] = pre_data.groupby(\"id\")['sales'].transform(lambda x: x.rolling(30).mean())\n",
    "\n",
    "    #for query data points keeping lag and rolling features same as previous 28 days lag and rolling features \n",
    "    pre_data.iloc[1941:,-11:] = pre_data.iloc[1913:1941,-11:].values\n",
    "    pre_data = pre_data[pre_data['d'] >= 1942]\n",
    "    pre_data.drop(['id', 'd','sales', 'wm_yr_wk'],axis = 1,inplace = True)\n",
    "\n",
    "    cat_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1','event_type_1', 'event_name_2', 'event_type_2']\n",
    "\n",
    "    for i in cat_cols:\n",
    "        if i in sales.columns:\n",
    "            dict_id = dict(enumerate(sales[i].cat.categories))\n",
    "        else:\n",
    "            dict_id = dict(enumerate(cal[i].cat.categories))\n",
    "        \n",
    "        keys = list(dict_id.keys())\n",
    "        values = list(dict_id.values())\n",
    "        og_values = list(pre_data[i].unique())\n",
    "        replace_values = []\n",
    "        for j in og_values:\n",
    "            if j == 'No_event':\n",
    "                replace_values.append(-1)\n",
    "            else:\n",
    "                replace_values.append(keys[values.index(j)])\n",
    "\n",
    "        pre_data[i].replace(og_values,replace_values,inplace = True)        \n",
    "            \n",
    "    model_file = open('cgb.pkl', 'rb')\n",
    "    model = pickle.load(model_file)\n",
    "\n",
    "    forecast_values = model.predict(pre_data)\n",
    "\n",
    "    return forecast_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a single  point as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>HOUSEHOLD_2_064_CA_1_evaluation</td>\n",
       "      <td>HOUSEHOLD_2_064</td>\n",
       "      <td>HOUSEHOLD_2</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 1947 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id          item_id      dept_id  \\\n",
       "1160  HOUSEHOLD_2_064_CA_1_evaluation  HOUSEHOLD_2_064  HOUSEHOLD_2   \n",
       "\n",
       "         cat_id store_id state_id  d_1  d_2  d_3  d_4  ...  d_1932  d_1933  \\\n",
       "1160  HOUSEHOLD     CA_1       CA    0    3    0    0  ...       1       1   \n",
       "\n",
       "      d_1934  d_1935  d_1936  d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "1160       2       0       0       0       1       2       1       0  \n",
       "\n",
       "[1 rows x 1947 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#picking a random data point\n",
    "data_point = sales.sample()\n",
    "data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64858867, 0.51179078, 0.36817153, 0.36198082, 0.3986266 ,\n",
       "       0.49778588, 0.38601405, 0.13291047, 0.11331706, 0.11591272,\n",
       "       0.10329076, 0.11125815, 0.14786973, 0.14553241, 0.09866009,\n",
       "       0.09450613, 0.08564963, 0.08564963, 0.08946307, 0.12483343,\n",
       "       0.12297282, 0.07890002, 0.08305397, 0.08305397, 0.07890002,\n",
       "       0.08686741, 0.12067947, 0.11881887])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_1(data_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a single  point as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['FOODS_2_366_CA_4_evaluation', 'FOODS_2_366', 'FOODS_2', ..., 0,\n",
       "        2, 1]], dtype=object)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_point = sales.sample().values\n",
    "data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40268731, 0.31032722, 0.32113162, 0.29916445, 0.29271972,\n",
       "       0.36386782, 0.33534181, 0.26243998, 0.1132565 , 0.09625875,\n",
       "       0.08740224, 0.09536964, 0.12965842, 0.13195177, 0.08740224,\n",
       "       0.08324829, 0.08740224, 0.08740224, 0.09121568, 0.12658604,\n",
       "       0.12472544, 0.08065263, 0.08480659, 0.08480659, 0.08065263,\n",
       "       0.08862002, 0.12243209, 0.12057148])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_1(data_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For multiple points as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['HOUSEHOLD_2_260_WI_2_evaluation', 'HOUSEHOLD_2_260',\n",
       "        'HOUSEHOLD_2', ..., 0, 1, 0],\n",
       "       ['HOBBIES_1_178_CA_2_evaluation', 'HOBBIES_1_178', 'HOBBIES_1',\n",
       "        ..., 0, 1, 2],\n",
       "       ['HOBBIES_1_255_WI_2_evaluation', 'HOBBIES_1_255', 'HOBBIES_1',\n",
       "        ..., 0, 1, 0],\n",
       "       ['FOODS_3_205_CA_2_evaluation', 'FOODS_3_205', 'FOODS_3', ..., 0,\n",
       "        1, 3],\n",
       "       ['HOBBIES_1_011_TX_3_evaluation', 'HOBBIES_1_011', 'HOBBIES_1',\n",
       "        ..., 0, 0, 1]], dtype=object)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#taking 5 random points\n",
    "\n",
    "data_points = sales.sample(5).values\n",
    "data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of data point 1 is \n",
      "[0.32184707 0.30285269 0.33149263 0.30774923 0.27452983 0.27434114\n",
      " 0.1583762  0.11150389 0.09654115 0.09913681 0.10329076 0.10662748\n",
      " 0.14786973 0.14553241 0.08564963 0.08149567 0.08564963 0.08564963\n",
      " 0.08946307 0.12483343 0.12297282 0.07890002 0.08305397 0.08305397\n",
      " 0.07890002 0.08686741 0.12067947 0.11881887]\n",
      "Results of data point 2 is \n",
      "[2.54046498 1.58383697 1.37242342 1.0724422  1.04058291 1.15509363\n",
      " 0.80415295 0.66389606 0.57562575 0.42986432 0.43401827 0.34892415\n",
      " 0.32507672 0.22247847 0.17097548 0.14541494 0.13655844 0.08307117\n",
      " 0.0868846  0.12225496 0.12039436 0.07632155 0.08047551 0.08047551\n",
      " 0.07632155 0.08428894 0.11810101 0.1162404 ]\n",
      "Results of data point 3 is \n",
      "[0.41827338 0.38673357 0.40310028 0.24996947 0.11873901 0.18173156\n",
      " 0.15764393 0.11540229 0.09580888 0.09377386 0.09792782 0.10589521\n",
      " 0.14713746 0.12483621 0.08491736 0.0807634  0.08491736 0.08491736\n",
      " 0.08873079 0.12410116 0.12224055 0.07816774 0.0823217  0.0823217\n",
      " 0.07816774 0.08613514 0.1199472  0.11808659]\n",
      "Results of data point 4 is \n",
      "[0.88398767 0.78955059 0.71632921 0.51055564 0.46344402 0.51019047\n",
      " 0.49883485 0.39914464 0.31775822 0.12229601 0.12181929 0.12978668\n",
      " 0.17102894 0.15191569 0.1004127  0.09625875 0.1004127  0.1004127\n",
      " 0.09121568 0.12658604 0.12472544 0.08065263 0.08480659 0.08480659\n",
      " 0.08065263 0.08862002 0.12243209 0.12057148]\n",
      "Results of data point 5 is \n",
      "[0.27198071 0.19014187 0.1805048  0.10239183 0.10518012 0.13954061\n",
      " 0.15990703 0.10239183 0.09776115 0.0807634  0.08491736 0.09288475\n",
      " 0.12254286 0.12946688 0.08491736 0.0807634  0.08491736 0.08491736\n",
      " 0.08873079 0.12410116 0.12224055 0.07816774 0.0823217  0.0823217\n",
      " 0.07816774 0.08613514 0.1199472  0.11808659]\n"
     ]
    }
   ],
   "source": [
    "pred_array = func_1(data_points)\n",
    "pred_array = np.reshape(pred_array, (-1, 28),order = 'F')\n",
    "for i in range(len(pred_array)):\n",
    "    print(f\"Results of data point {i+1} is \\n{pred_array[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_2(data_point):\n",
    "    if type(data_point) != pd.core.frame.DataFrame:\n",
    "        data_point = pd.DataFrame(data_point,columns = sales.columns)\n",
    "    \n",
    "    #getting previous 30 days data also for calculating lags and rolling feature \n",
    "    d_cols = [d for d in sales.columns if 'd_' in d]\n",
    "    data = data_point.drop(d_cols[:1883],axis = 1)\n",
    "    \n",
    "    #Converting from wide form to long form\n",
    "    pre_data = pd.melt(data, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
    "                      var_name='d', value_name='sales')\n",
    "    \n",
    "    #combining the dataset \n",
    "    pre_data = pd.merge(pre_data, cal, on='d', how='left')\n",
    "    pre_data = pd.merge(pre_data, price, on=['store_id','item_id','wm_yr_wk'], how='left')\n",
    "    \n",
    "    #fil the missing sell price values by mean imputaion\n",
    "    pre_data[\"sell_price\"].fillna(pre_data.groupby(\"id\")[\"sell_price\"].transform(\"mean\"), inplace=True)\n",
    "    \n",
    "    pre_data.drop(columns=[\"date\",\"weekday\"], inplace=True)\n",
    "    pre_data['d'] = pre_data['d'].apply(lambda a: a.split('_')[1]).astype(np.int16)\n",
    "\n",
    "    #calculating lags feature\n",
    "    lags = [1,2,3,5,7,14,21,28]\n",
    "    for lag in lags:\n",
    "        pre_data[\"lag_\" + str(lag)] = pre_data.groupby(\"id\")[\"sales\"].shift(lag).astype(np.float16)\n",
    "\n",
    "    #calculating rolling features\n",
    "    pre_data['rolling_mean_10'] = pre_data.groupby(\"id\")['sales'].transform(lambda x: x.rolling(10).mean())\n",
    "    pre_data['rolling_mean_20'] = pre_data.groupby(\"id\")['sales'].transform(lambda x: x.rolling(20).mean())\n",
    "    pre_data['rolling_mean_30'] = pre_data.groupby(\"id\")['sales'].transform(lambda x: x.rolling(30).mean())\n",
    "\n",
    "    pre_data = pre_data[pre_data['d'] >= 1914]\n",
    "    pre_data.drop(['id', 'd','sales', 'wm_yr_wk'],axis = 1,inplace = True)\n",
    "\n",
    "    cat_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1','event_type_1', 'event_name_2', 'event_type_2']\n",
    "\n",
    "    for i in cat_cols:\n",
    "        if i in sales.columns:\n",
    "            dict_id = dict(enumerate(sales[i].cat.categories))\n",
    "        else:\n",
    "            dict_id = dict(enumerate(cal[i].cat.categories))\n",
    "        \n",
    "        keys = list(dict_id.keys())\n",
    "        values = list(dict_id.values())\n",
    "        og_values = list(pre_data[i].unique())\n",
    "        replace_values = []\n",
    "        for j in og_values:\n",
    "            if j == 'No_event':\n",
    "                replace_values.append(-1)\n",
    "            else:\n",
    "                replace_values.append(keys[values.index(j)])\n",
    "\n",
    "        pre_data[i].replace(og_values,replace_values,inplace = True)        \n",
    "            \n",
    "    model_file = open('cgb.pkl', 'rb')\n",
    "    model = pickle.load(model_file)\n",
    "\n",
    "    fore_array = model.predict(pre_data)\n",
    "    fore_array = np.reshape(fore_array, (-1, 28),order = 'F')\n",
    "    preds_val = pd.DataFrame(fore_array,columns = d_cols[-28:])\n",
    "    train_df = sales.iloc[:,:-28]\n",
    "    val_df = sales.iloc[:,-28:]\n",
    "    evaluator = WRMSSEEvaluator(train_df, val_df, cal, price)\n",
    "    score = evaluator.score(preds_val)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:35<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final metric: 0.6710504162888493\n"
     ]
    }
   ],
   "source": [
    "#passing data of all items of all stores because the performance metric is based on hierarchical forecasting scoring.\n",
    "#it aggregates scores of all heirarchical levels \n",
    "\n",
    "final_metric = func_2(sales.values) \n",
    "print(f\"The final metric: {final_metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
